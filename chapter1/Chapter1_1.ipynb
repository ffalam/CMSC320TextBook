{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### **How Data Science Relates to AI, ML, Deep Learning, and Statistics**\n",
    "\n",
    "Although the terms *AI*, *machine learning*, *deep learning*, *statistics*, and *data science* are often used interchangeably, they refer to different but overlapping ideas. Broadly:\n",
    "\n",
    "- **AI** focuses on techniques that enable computers to mimic or approximate human intelligence. **Example:** a chatbot that carries on a conversation or an agent that plays chess.\n",
    "- **Machine Learning (ML)** is a subset of AI focused on learning patterns from data. **Example:** predicting house prices based on past sales.\n",
    "- **Deep Learning (DL)** is a subset of ML that uses neural networks with many layers. **Example:** recognizing objects in images using convolutional neural networks.\n",
    "- **Statistics** provides mathematical tools for describing data and making inferences. **Example:** estimating the average test score of a population from a sample.\n",
    "- **Data Science** connects statistics and computing to manage, analyze, visualize, and extract insight from data. **Example:** analyzing customer data to recommend products or forecast demand.\n",
    "\n",
    "These fields overlap in practice, especially in modern applications where statistical reasoning, computational tools, and learning algorithms work together.\n",
    "\n",
    "The diagrams below show two common ways these fields are represented visually. Although simplified, they highlight the core relationships and boundaries.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"https://ai-fall2023.ai2es.org/wp-content/uploads/2023/07/AI_VennDiagram_v2.png\" width=\"400\"><br>\n",
    "      <em>Figure 4A: Venn diagram showing relationships between AI, ML, and related fields. Source: AI Fall 2023 (ai-fall2023.ai2es.org)</em>\n",
    "    </td>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"https://studyopedia.com/wp-content/uploads/2022/12/Data-Science-VS-Machine-Learning-VS-Artificial-Intelligence-vs-Deep-Learning-Studyopedia.png\" width=\"500\"><br>\n",
    "      <em>Figure 4B: Comparison of Data Science, ML, AI, and Deep Learning. Source: Studyopedia</em>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "<!-- <p align=\"center\">\n",
    "  <img src=\"YOUR_IMG_LINK_HERE\" width=\"700\"><br>\n",
    "  <em>Figure: Relationship between AI, ML, Deep Learning, Statistics, and Data Science.</em>\n",
    "</p> -->\n"
   ],
   "metadata": {
    "id": "_KlaDHT40214"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Brief History of Data Science**\n",
    "\n",
    "Data science is a fairly new field. The term \u201cdata science\u201d became popular in the early 2000s to describe a modern profession focused on using math, statistics, and computing to make sense of large and complex datasets, often called \u201cbig data.\u201d Although data science feels like a modern invention, its roots stretch back much further than most people realize. At its core, the field grew out of two enduring human goals: to **collect data** and to **make sense of it**.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*24TM7X287gys5Ww3lT0NBw.png\" width=\"650\"><br>\n",
    "  <em>Figure 5: A high-level timeline of ideas that shaped the modern field of data science. Source: Adapted from Florian Huber, images of Bayes and Gauss from Wikipedia.</em>\n",
    "</p>\n",
    "\n",
    "#### **Early Foundations: Statistics Meets Computation (1900s\u20131950s)**\n",
    "\n",
    "In the early 20th century, statistics emerged as a formal discipline, giving scientists tools for summarizing and analyzing data. The arrival of digital computers in the mid-1900s transformed these ideas: statistical methods could now be implemented at speed and at scale, laying the groundwork for computational approaches to data.\n",
    "\n",
    "#### **Data Analysis as a Scientific Activity (1960s)**\n",
    "\n",
    "In the 1960s, the idea of treating **data analysis itself as a scientific activity** began to take shape. The statistician **John Tukey** argued that data analysis deserved a role alongside theory and mathematics as a way of extracting meaning from raw information.\n",
    "\n",
    "#### **Knowledge Discovery in Databases (1970s\u20131980s)**\n",
    "\n",
    "As more data began to be stored electronically in the 1970s and 1980s, research communities focused on how to organize and learn from large datasets. This gave rise to **KDD** (Knowledge Discovery in Databases), which emphasized finding patterns and insights from stored data; an early precursor to modern data mining and machine learning.\n",
    "\n",
    "#### **The Phrase \u201cData Science\u201d Appears (1974)**\n",
    "\n",
    "Around the same time, in 1974, **Peter Naur** used the phrase *data science* to describe the emerging combination of computing and data analysis; notably before the term was widely adopted in industry or academia.\n",
    "\n",
    "#### **Data Mining Becomes a Discipline (1990s)**\n",
    "\n",
    "By the 1990s, advances in databases, algorithms, and computing infrastructure pushed data analysis further, helping establish data mining as a research discipline with conferences, communities, and standardized practices.\n",
    "\n",
    "#### **The Modern Term Takes Shape (Early 2000s)**\n",
    "\n",
    "In the early 2000s, the term **data science** began to spread more broadly. In 2001, **William S. Cleveland** proposed redefining statistics for the era of computing and large-scale data. Meanwhile, the internet, search engines, and digital storage were producing unprecedented quantities of information \u2014 early signals of what would soon be called **big data**.\n",
    "\n",
    "#### **Mainstream Adoption and Industry Growth (2010s)**\n",
    "\n",
    "In the 2010s, data science entered mainstream awareness. Massive data streams from the web, mobile devices, sensors, and business systems demanded new methods to store, process, analyze, and model information. Organizations began adopting data science to improve decision-making, automate tasks, and gain competitive advantage.\n",
    "\n",
    "\n",
    "### **The Big Data Era Changes Everything**\n",
    "\n",
    "A major turning point came when technology companies such as **Google**, **Amazon**, **Facebook**, **Netflix**, and **LinkedIn** began collecting enormous amounts of behavioral data: clicks, searches, purchases, ratings, and social interactions. Traditional tools could not handle data at this scale, leading to new distributed systems such as **Hadoop**, **MapReduce**, and later **Spark**.\n",
    "\n",
    "> ### **What Is Big Data?**\n",
    ">\n",
    "> \u201cBig data\u201d refers to data that is so large, fast, or complex that traditional tools cannot handle it easily. When people first started using the term, big data was explained with **three V\u2019s**:\n",
    ">\n",
    "> - **Volume** \u2014 a very large amount of data  \n",
    "> - **Velocity** \u2014 how quickly data is created and needs to be processed  \n",
    "> - **Variety** \u2014 data in many different forms (text, images, logs, videos, etc.)  \n",
    ">\n",
    "> As more companies and researchers began working with big data, they noticed that size alone did not explain the full challenge. Later, two more V\u2019s were added:\n",
    ">\n",
    "> - **Veracity** \u2014  refers to the quality of data that is being analyzed, as data may be messy, uncertain, or low quality.\n",
    "> - **Value** \u2014 making sure the data is useful and worth analyzing .\n",
    ">\n",
    "> These V\u2019s help show not only what big data looks like, but also why it matters. In some discussions, additional V\u2019s such as variability, visualization, and validity are also mentioned, although the five V\u2019s remain the most commonly used definition.\n",
    "<!--\n",
    "<center>\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*YejjU_69ffDyrC0z-X9jYQ.jpeg\" width=\"450\">\n",
    "  <br>\n",
    "  <em>Figure 6. A visual depiction of the 5 V\u2019s of Big Data: Volume, Velocity, Variety, Veracity, and Value.<br>\n",
    "  Source: miro.medium.com </em>\n",
    "</center> -->\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*YejjU_69ffDyrC0z-X9jYQ.jpeg\" width=\"450\"><br>\n",
    "      <em>Figure 6-A. A Visual Depiction of the 5 V\u2019s of Big Data.<br>\n",
    "      Source: miro.medium.com.</em>\n",
    "    </td>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"https://blogs-images.forbes.com/dam/imageserve/5bfccfcd31358e5b4337d5e1/0x0.png?cropX1=-1&cropY1=-1&cropX2=-1&cropY2=-1&quality=75&fit=&background=000000&uri=tomcoughlin/files/2018/11/Growth-of-Datasphere.png\" width=\"450\"><br>\n",
    "      <em>Figure 6-B. Growth of the Global Datasphere.<br>\n",
    "      Source: Forbes (Tom Coughlin).</em>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "What made this period transformative was not just the size of the data, but how it was used. Companies learned to use data to recommend, rank, predict, and personalize services automatically. Data became a competitive advantage. As the value became clear, other industries, from finance and healthcare to transportation and retail, adopted similar strategies. By the late 2010s, data science had shifted from a niche specialty to a mainstream discipline practiced across many sectors.\n",
    "\n",
    "Today, data science sits at the intersection of **statistics**, **computational methods**, and **domain knowledge**, supported by machine learning and deep learning. It continues to evolve as new tools reshape how we ask questions, what problems we can solve, and how data informs society.\n",
    "\n",
    "---\n",
    "\n",
    "### **Quick Timeline Summary**\n",
    "\n",
    "> **Early 1900s** \u2014 Formal statistical methods emerge  \n",
    "> **1960s** \u2014 Tukey emphasizes data analysis as a scientific activity  \n",
    "> **1970s\u20131980s** \u2014 Electronic storage + computing \u2192 large datasets  \n",
    "> **1990s** \u2014 KDD and data mining research communities develop  \n",
    "> **Early 2000s** \u2014 \u201cData science\u201d becomes a formal term (Cleveland 2001)  \n",
    "> **2010s** \u2014 Big data, industry adoption, and machine learning growth  \n",
    "> **Today** \u2014 Data science integrates computation, ML/DL, and domain context  \n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "Ik94TUR2GJrV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **The Data Science Life Cycle**\n",
    "\n",
    "Now that we have a sense of where data science came from, it is helpful to look at how it actually works in practice. Different organizations may have their own variations, but most data science projects follow a similar life cycle. The goal of this process is to move from raw data to insight, decisions, and value.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://drive.google.com/uc?export=view&id=17LAur9qfw0t_7SZrsyTb0MIm64oVRo1J\" width=\"650\"><br>\n",
    "  <em>Figure 7: Data Science Life Cycle (DSLC). Source: Dr. John Dickerson</em>\n",
    "</p>\n",
    "\n",
    "### **Step I: Data Collection**\n",
    "\n",
    "The life cycle typically begins with **data collection** \u2014 gathering information from available sources. Data may come from sensors, surveys, transaction systems, public datasets, or internal databases. Data can be:\n",
    "\n",
    "- **Primary data**, collected directly for the project (e.g., surveys, experiments, field observations, sensors).\n",
    "- **Secondary data**, which already exists and was collected for another purpose (e.g., public datasets, social media, administrative records).\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://cdn.educba.com/academy/wp-content/uploads/2023/09/Data-Collection-Methods.png\" width=\"700\"><br>\n",
    "  <em>Figure 8: Common data collection methods. Source: EDUCBA</em>\n",
    "</p>\n",
    "\n",
    "> **Why it matters:** Primary data is highly relevant but often costly and time-consuming. Secondary data is faster and cheaper to obtain, but may not perfectly match the analysis goals or may require additional cleaning and assumptions.\n",
    "\n",
    "In practice, analysts often blend both types. For example, a public health researcher might conduct new surveys (**primary**) while using census data for context (**secondary**).\n",
    "\n",
    "### **Step II: Data Processing**\n",
    "\n",
    "Once collected, data rarely arrives in clean form. **Data processing** involves organizing, cleaning, and transforming raw data so that it can support analysis. This may include fixing errors, handling missing values, resolving inconsistencies, or merging multiple datasets. Feature engineering often occurs here to create variables that better capture relevant relationships.\n",
    "\n",
    "> **Why it matters:** Clean, well-structured data increases the chance of extracting meaningful insights and prevents misleading results. Processing often consumes a large portion of a project.\n",
    "\n",
    "Analysts frequently iterate between processing and earlier steps as data gaps or issues emerge.\n",
    "\n",
    "### **Step III: Exploratory Data Analysis (EDA)**\n",
    "\n",
    "With processed data, **EDA** helps analysts explore patterns, distributions, correlations, and anomalies. Techniques include descriptive statistics and visualizations. For example, an analyst might examine the relationship between study hours and test scores by calculating average scores, looking at score ranges, and noticing that some students performed exceptionally well.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250806130212248170/exploratory_data_analysis_eda_2.webp\" width=\"700\"><br>\n",
    "  <em>Figure 9: Illustrative example of Exploratory Data Analysis (EDA). Source: GeeksforGeeks.</em>\n",
    "</p>\n",
    "\n",
    "\n",
    "Visualization can make these patterns clearer. Plotting study hours against average scores may reveal that students who study more tend to achieve higher scores\u2014a useful pattern that might inform later modeling.\n",
    "\n",
    "> **Why it matters:** EDA guides the rest of the project. It shapes hypotheses, informs modeling choices, and reveals data limitations.\n",
    "\n",
    "EDA often sends analysts back to processing or even data collection to refine inputs.\n",
    "\n",
    "\n",
    "### **Step IV: Analysis and Modeling**\n",
    "\n",
    "Next, the project moves into **analysis and modeling**, which may involve statistical inference or machine learning. The goal is to either explain current patterns or predict future outcomes, depending on the context.\n",
    "\n",
    "> **Why it matters:** Modeling provides structure and rigor. It allows relationships to be quantified and predictions to be made\u2014insights that may be difficult to detect through raw inspection alone.\n",
    "\n",
    "### **Step V: Insight and Decision-Making**\n",
    "\n",
    "Finally, results must be translated into decisions, recommendations, or actions. This may involve policy suggestions, business strategies, scientific findings, or product improvements. Clear communication is essential, as stakeholders often are not technical experts.\n",
    "\n",
    "> **Why it matters:** The value of data science is realized only when insights lead to decisions. A sophisticated model has little impact if it does not shape understanding or drive action.\n",
    "\n",
    "Decision-making often generates new questions, sending the project back to earlier steps.\n",
    "\n",
    "### **A Simple Example**\n",
    "\n",
    "Consider a city trying to improve traffic flow. It might collect data from sensors (**collection**), clean and merge logs (**processing**), explore patterns (**EDA**), build a model to predict congestion (**analysis and modeling**), and adjust traffic policies (**decision-making**). After implementation, new data arrives and the cycle repeats.\n",
    "\n",
    "### **A Key Reminder**\n",
    "\n",
    "**Data science is not a strictly one-way, linear process; it is dynamic, iterative, and adaptive.**  \n",
    "In real projects, analysts often loop backward \u2014 re-cleaning data, adjusting models, re-thinking assumptions, or requesting new data. The process behaves more like a feedback loop than an assembly line.\n"
   ],
   "metadata": {
    "id": "mBq8jSFCx517"
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}