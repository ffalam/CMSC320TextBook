{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbCmWrjfZ5GT"
   },
   "source": [
    "# How Decision Trees Decide Which Feature to Split (Using Entropy and Information Gain)\n",
    "\n",
    "Decision Trees use **Entropy** and **Information Gain** to decide **how to split the data at each node**. The goal is to create **subgroups (child nodes) that are as pure as possible** with respect to the target variable.\n",
    "\n",
    "> The core idea of a Decision Tree is **recursive splitting**: at each node, we pick the **feature that best separates the data** into purer subgroups.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step Process**\n",
    "\n",
    "1. **Start at the root node**  \n",
    "   - The root node contains the entire dataset.  \n",
    "   - Calculate the **Entropy** of the root node to measure the overall uncertainty.\n",
    "\n",
    "<!-- 2. **Evaluate all features for splitting**  \n",
    "   - For each feature, simulate splitting the data according to its possible values.  \n",
    "   - Compute the **weighted entropy** of the resulting child nodes.  \n",
    "   - Calculate the **Information Gain** for that feature:  \n",
    "$IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\, Entropy(S_v)$ -->\n",
    "\n",
    "2. **Evaluate each feature**  \n",
    "   - For each feature, split the dataset according to its possible values.  \n",
    "   - Compute the **entropy of each child node** and the **weighted average entropy**:\n",
    "$$\n",
    "Entropy_{after\\_split} = \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\, Entropy(S_v)\n",
    "$$\n",
    "\n",
    "   - Calculate **Information Gain**:\n",
    "\n",
    "$$\n",
    "IG(S, A) = Entropy(S) - Entropy_{after\\_split}\n",
    "$$\n",
    "\n",
    "   - A **higher IG** means the feature produces **purer child nodes**.\n",
    "\n",
    "3. **Choose the best feature to split**  \n",
    "   - Compare the Information Gain of all features.  \n",
    "   - Select the **feature with the highest IG** for splitting.  \n",
    "   - This ensures each split reduces the most uncertainty.\n",
    "      - Creates child nodes that are **more homogeneous** than the parent\n",
    "\n",
    "\n",
    "<!-- 3. **Choose the best feature to split**  \n",
    "   - The feature with the **highest Information Gain** is selected to split the current node.  \n",
    "   - This creates child nodes that are **more homogeneous** than the parent. -->\n",
    "\n",
    "\n",
    "4. **Repeat recursively for each child node**  \n",
    "   - Treat each child node as a new node and repeat Steps 1â€“3.  \n",
    "   - Continue until:\n",
    "     - Nodes are **pure** (entropy = 0), or  \n",
    "     - A **stopping condition** is reached (e.g., max depth, min samples per node).\n",
    "\n",
    "\n",
    "<!-- 4. **Repeat recursively for each child node**  \n",
    "   - For each child node, repeat the process: calculate entropy, evaluate features, and split using the feature with highest information gain.  \n",
    "   - Continue until:  \n",
    "     - Nodes are **pure** (all samples in the node belong to the same class), or  \n",
    "     - A **stopping criterion** is reached (e.g., max depth, min samples per node). -->\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Idea**\n",
    "\n",
    "- **Entropy** tells us how **mixed** the node is.  \n",
    "- **Information Gain** tells us how **good a feature is at reducing uncertainty**.  \n",
    "- Decision Trees **always pick the feature that maximizes Information Gain** to make the next split.  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
