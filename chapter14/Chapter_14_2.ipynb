{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhVg7lEuQKNT"
   },
   "source": [
    "## Information Gain (IG)\n",
    "\n",
    "Once we know how to calculate **Entropy**, we can measure how much **uncertainty** (or impurity) is reduced when we split a dataset using a feature. This reduction is called **Information Gain**.\n",
    "\n",
    "Information Gain measures the **reduction in entropy** achieved by a split. It tells us **how much uncertainty (impurity) is removed** by a particular split.  \n",
    "\n",
    "### **Definition**\n",
    "\n",
    "Formally, suppose a split divides the parent node $S$ into $V$ subsets:  \n",
    "$$\n",
    "S_1, S_2, ..., S_V\n",
    "$$\n",
    "\n",
    "Let:  \n",
    "- $|S|$ = total number of samples in the parent node  \n",
    "- $|S_v|$ = number of samples in child node $S_v$  \n",
    "\n",
    "Information Gain for a feature **A** is defined as:\n",
    "\n",
    "$$\n",
    "IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\, Entropy(S_v)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $S$ = parent node (original dataset)  \n",
    "- $A$ = feature used for splitting  \n",
    "- $S_v$ = subset of $S$ corresponding to value $v$ of feature $A$  \n",
    "- $|S|$, $|S_v|$ = number of samples in $S$ and $S_v$, respectively  \n",
    "\n",
    "The algorithm selects the **feature with the highest Information Gain** for splitting.\n",
    "\n",
    "---\n",
    "\n",
    "### **Intuition**\n",
    "\n",
    "Information Gain measures how much a split **reduces impurity**:\n",
    "- A **high IG** means the feature created **purer subsets** (good split).  \n",
    "- A **low IG** means the feature did not reduce impurity much (poor split).  \n",
    "\n",
    "Decision Trees choose the feature with the **highest Information Gain** at each step.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "\n",
    "Suppose a dataset has 10 samples:  \n",
    "- 6 **Pass**, 4 **Fail**  \n",
    "\n",
    "Then the initial (parent) entropy is:\n",
    "\n",
    "$$\n",
    "Entropy(S) = -\\left( \\frac{6}{10}\\log_2\\frac{6}{10} + \\frac{4}{10}\\log_2\\frac{4}{10} \\right) \\approx 0.971\n",
    "$$\n",
    "\n",
    "Now we split based on **Feature A** (e.g., “Study Hours”) which has two possible values:  \n",
    "- **A = High:** 5 samples → 4 Pass, 1 Fail  \n",
    "- **A = Low:** 5 samples → 2 Pass, 3 Fail  \n",
    "\n",
    "Compute entropy for each group:\n",
    "\n",
    "$$\n",
    "Entropy(S_{High}) = -\\left( \\frac{4}{5}\\log_2\\frac{4}{5} + \\frac{1}{5}\\log_2\\frac{1}{5} \\right) \\approx 0.722\n",
    "$$\n",
    "\n",
    "$$\n",
    "Entropy(S_{Low}) = -\\left( \\frac{2}{5}\\log_2\\frac{2}{5} + \\frac{3}{5}\\log_2\\frac{3}{5} \\right) \\approx 0.971\n",
    "$$\n",
    "\n",
    "Then the weighted average entropy after splitting is:\n",
    "\n",
    "$$\n",
    "Entropy_{after} = \\frac{5}{10}(0.722) + \\frac{5}{10}(0.971) = 0.847\n",
    "$$\n",
    "\n",
    "Finally, the **Information Gain** is:\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KjVvgo1Ks0WJb6i3tHwdBA.png\" alt=\"Pandas Illustration\" width=\"600\">\n",
    "</center>\n",
    "\n",
    "\n",
    "$$\n",
    "IG(S, A) = 0.971 - 0.847 = 0.124\n",
    "$$\n",
    "\n",
    "So, splitting on **Feature A** reduces uncertainty by **0.124 bits**.\n",
    "\n",
    "### **Additional Example:**\n",
    "Example illustrating the calculation of information gain. Source: Hendler 2018, slide 46.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://devopedia.org/images/article/168/4250.1555312917.jpg\" alt=\"Pandas Illustration\" width=\"600\">\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "### **Try it Yourself**\n",
    "\n",
    "If you have another feature **B** that splits the same data into:\n",
    "- **B = Yes:** 6 samples (5 Pass, 1 Fail)  \n",
    "- **B = No:** 4 samples (1 Pass, 3 Fail)\n",
    "\n",
    "Try calculating:\n",
    "\n",
    "$$\n",
    "Entropy(S_{Yes}), \\; Entropy(S_{No}), \\; \\text{and} \\; IG(S, B)\n",
    "$$\n",
    "\n",
    "Which feature gives a higher Information Gain — **A** or **B**?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def entropy(class_counts):\n",
    "    \"\"\"\n",
    "    Compute entropy for a list of class counts.\n",
    "    class_counts: list of counts for each class\n",
    "    \"\"\"\n",
    "    total = sum(class_counts)\n",
    "    entropy_value = 0\n",
    "    for count in class_counts:\n",
    "        if count == 0:  # avoid log(0)\n",
    "            continue\n",
    "        p = count / total\n",
    "        entropy_value -= p * math.log2(p)\n",
    "    return entropy_value\n",
    "\n",
    "def information_gain(parent_counts, split_counts_list):\n",
    "    \"\"\"\n",
    "    Compute Information Gain.\n",
    "    parent_counts: list of counts in the parent node\n",
    "    split_counts_list: list of lists, each sublist contains counts in a child node\n",
    "    \"\"\"\n",
    "    total_parent = sum(parent_counts)\n",
    "    parent_entropy = entropy(parent_counts)\n",
    "\n",
    "    weighted_entropy = 0\n",
    "    for counts in split_counts_list:\n",
    "        weight = sum(counts) / total_parent\n",
    "        weighted_entropy += weight * entropy(counts)\n",
    "\n",
    "    ig = parent_entropy - weighted_entropy\n",
    "    return ig\n",
    "\n",
    "# Example: Feature A split\n",
    "# Parent node: 6 Pass, 4 Fail\n",
    "parent = [6, 4]\n",
    "\n",
    "# Feature A splits:\n",
    "# High: 4 Pass, 1 Fail\n",
    "# Low: 2 Pass, 3 Fail\n",
    "split_A = [[4, 1], [2, 3]]\n",
    "\n",
    "print(\"Information Gain for Feature A:\", round(information_gain(parent, split_A), 3))\n",
    "\n",
    "# Feature B splits:\n",
    "# Yes: 5 Pass, 1 Fail\n",
    "# No: 1 Pass, 3 Fail\n",
    "split_B = [[5, 1], [1, 3]]\n",
    "\n",
    "print(\"Information Gain for Feature B:\", round(information_gain(parent, split_B), 3))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
