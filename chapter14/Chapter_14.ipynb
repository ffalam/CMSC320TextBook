{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction: The Power of Simple Questions\n",
        "\n",
        "Imagine you are a doctor diagnosing a patient. You don't run every test simultaneously. Instead, you ask a sequence of questions: \"Does the patient have a fever?\" If yes, you ask, \"Is there a rash?\" If no, you might ask, \"Is the pain localized?\" This hierarchical process of asking questions to narrow down possibilities is the very essence of a Decision Tree.\n",
        "\n",
        "A Decision Tree is a **non-parametric** (**no** assumptions about data distribution or functional form, allowing them to learn patterns directly from the data), **supervised learning** algorithm used for both classification and regression tasks. Its model forms a tree-like structure, mimicking human decision-making processes.\n",
        "\n",
        "\n",
        "\n",
        "> **Parameteric vs Non-Parametric**: A Decision Tree (DT) is non-parametric, meaning it makes no assumptions about data distribution or functional form and has no fixed number of parameters—its complexity (splits, depth, etc.) adapts to the data; unlike parametric models (e.g., linear or logistic regression) that assume a specific functional form and have fixed parameters.\n",
        "\n",
        "\n",
        "They **mimic human decision-making** by breaking down complex decisions into a series of simpler questions. A Decision Tree represents decisions in a flowchart-like, hierarchical structure that models possible consequences—including outcomes, costs, and probabilities and is highly interpretable.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20221212133528/dcsion.png\" alt=\"Pandas Illustration\" width=\"600\">\n",
        "</center>\n",
        "\n",
        "They are especially popular because they require minimal data preprocessing, can handle both numerical and categorical data, and provide models that are easily interpretable.\n",
        "\n",
        "\n",
        "It has many strengths:\n",
        "\n",
        "* **Interpretability:** The resulting model is a white box. The rules for making a prediction can be easily understood and explained, even to non-experts.\n",
        "\n",
        "* **Few Data Preprocessing Requirements:** They require little data preparation (e.g., no need for feature scaling or normalization).\n",
        "\n",
        "* **Handles Mixed Data Types:** They can work with both numerical and categorical data.\n",
        "\n",
        "* **Non-Linearity:** They can capture complex non-linear relationships between features and the target variable.\n",
        "\n",
        "\n",
        "However, this simplicity can also be a weakness because it often leads to **overfitting**. When a tree grows too deep, it starts to learn the noise in the training data, not just the underlying pattern, which causes poor performance on new data.\n",
        "\n",
        "In this chapter, we will break down how the **Decision Tree algorithm works**, understand how to create **good splitting questions**, learn how to **prune** an overly complex tree, and see how Decision Trees serve as the foundation for advanced ensemble methods like Random Forests and Gradient Boosting Machines"
      ],
      "metadata": {
        "id": "RQccx7lY-02X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Basic Concepts of Decision Tree\n",
        "\n",
        "Next, we will learn some basic concepts related to decision tree.\n",
        "\n",
        "## Structure of a Decision Tree and Terminology\n",
        "\n",
        "To understand how a tree is built, we must first understand its components. A Decision Tree is a graph composed of:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://365datascience.com/resources/blog/rr6cuudl59r-decision-trees-image1.png\" alt=\"Pandas Illustration\" width=\"600\">\n",
        "</center>\n",
        "\n",
        "\n",
        "* **Root Node:** The topmost node, representing the entire dataset. It is the starting point for the splitting process.\n",
        "\n",
        "* **Internal Nodes (Decision Nodes):** Nodes that represent a decision point or a test on a specific feature. Each internal node splits the data into two or more branches.\n",
        "\n",
        "* **Branches (Edges):** The outcome of a decision node test, leading to the next node.\n",
        "\n",
        "* **Leaf Nodes (Terminal Nodes):** The final nodes in the tree that do not split further. Each leaf node represents a class label (in classification) or a continuous value (in regression) which is the final prediction.\n",
        "\n",
        "The path from the root to a leaf represents a series of conjunctions (AND operations) of the conditions at the internal nodes, forming a classification rule.\n",
        "\n",
        "### More Terminology\n",
        "\n",
        "* **Sub-Tree:** A portion of the main tree.\n",
        "\n",
        "* **Splitting:** The process of dividing a node into two or more sub-nodes.\n",
        "\n",
        "* **Pruning:** The process of removing branches to reduce the complexity of the tree and prevent overfitting (we will talk about it later in this chapter).\n",
        "\n",
        "\n",
        "### **Example**\n",
        "\n",
        "Consider the classic “Play Tennis” dataset, where we want to predict whether to play tennis based on weather conditions (Outlook, Temperature, Humidity, Windy).\n",
        "\n",
        "A simplified decision tree might look like:\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://spotintelligence.com/wp-content/uploads/2024/05/decision-tree-example.jpg\" alt=\"Pandas Illustration\" width=\"600\">\n",
        "</center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N3T8TkBjBdfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: The Tree-Building Algorithm: A Greedy, Recursive Process\n",
        "\n",
        "The construction of a Decision Tree is a **greedy**, **recursive**, and **top-down** process, known as Recursive Binary Splitting (or partitioning).\n",
        "\n",
        "\n",
        "> **Key Idea:** The core idea behind building a decision tree is **recursive partitioning**; repeatedly splitting the dataset based on an attribute that maximizes separation between classes (or minimizes error in regression).\n",
        "\n",
        "\n",
        "\n",
        "## Detailed Concept of Decision Tree Learning Algorithm:\n",
        "\n",
        "* **Greedy:** At each node, the algorithm selects the locally optimal split without considering the overall tree structure or future splits. It makes the best decision it can at that moment.\n",
        "\n",
        "* **Recursive:** The process is repeated for each subset of the data created by previous splits.\n",
        "\n",
        "* **Top-Down:** It begins at the root and successively splits the dataset into smaller groups.\n",
        "\n",
        "\n",
        "### The Core Algorithm (**ID3** and its successors **C4.5**, **CART**):\n",
        "\n",
        "The **Iterative Dichotomiser 3 (ID3)** algorithm is a decision tree induction algorithm that constructs decision trees using a top-down approach. The main goal of ID3 is to find the most informative attributes at each step to create partitions that yield the **highest information gain** or **decrease in impurity**.\n",
        "\n",
        "Let's break down the ID3 algorithm into a step-by-step process:\n",
        "\n",
        "- ➤ 1. **Start** at the root node with the entire dataset.\n",
        "\n",
        "- ➤ 2. **For each feature in the dataset**, calculate a *\"goodness\" metric* (e.g., Information Gain, Gini Impurity) for all possible split points.\n",
        "\n",
        "> In this chapter, we will focus on \"Entropy\" and \"Information Gain\".\n",
        "\n",
        "- ➤ 3. Select the feature and split point that **maximizes this \"goodness\" metric**. This becomes the splitting rule for the node.\n",
        "\n",
        "- ➤ 4. **Split the dataset at the current node** into child nodes based on the selected rule.\n",
        "\n",
        "The process recursively **repeat steps 2-4**S for each child node until **a stopping criterion** is met.\n",
        "\n",
        "**Common stopping criteria** include:\n",
        "\n",
        "- The node is **\"pure\"** (homogeneous: all samples belong to the same class).\n",
        "- No more attributes remain for further splitting.\n",
        "- A stopping condition (like a predefined maximum depth) is reached.\n",
        "- The node contains fewer samples than the predefined minimum threshold.\n",
        "- A split does not produce a significant improvement (minimum gain threshold)."
      ],
      "metadata": {
        "id": "EmT0DkrTLJFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting Criteria (Attribute Selection Measures):\n",
        "\n",
        "The algorithm needs a metric to determine the **\"best\" split**. For classification trees, the goal is to **maximize the homogeneity** of the resulting subsets. The main metrics are **Entropy** and **Information Gain** (Used in ID3, C4.5).\n",
        "\n",
        "## Entropy\n",
        "\n",
        "Entropy measures the randomness or **impurity** in the dataset.\n",
        "\n",
        "\n",
        "\n",
        "> In information theory, the entropy of a random variable is the average level of \"information\", \"surprise\", or \"uncertainty\" inherent to the variable's possible outcomes.\n",
        "\n",
        "In a decision tree, a node that contains a mix of different outcomes (for example, 2 Pass and 2 Fail) has **higher entropy**, meaning more disorder or uncertainty. If a node has only one type of outcome (for example, all Pass or all Fail), it is **pure and has low entropy**.\n",
        "\n",
        "- **Maximum entropy = 1** → occurs when the outcomes are equally mixed (e.g., 50% Pass, 50% Fail).\n",
        "\n",
        "- **Minimum entropy = 0** → occurs when all outcomes are the same (e.g., 100% Pass or 100% Fail).\n",
        "\n",
        "> In short: Mixed outcomes → high entropy (uncertain) and Single outcome → low entropy (certain)\n",
        "\n",
        "\n",
        "<!-- <center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1358/format:webp/1*E2qTQ1NSzy2Zb6YC8oqDvA.png\" alt=\"Pandas Illustration\" width=\"600\">\n",
        "</center> -->\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*ODTVm0g4PkSZnDMW91y_Qw.png\" alt=\"Pandas Illustration\" width=\"600\">\n",
        "</center>\n",
        "\n",
        "The first thing to understand about Decision Trees is that they split the data **based on the input features into smaller groups** that are **more homogeneous (or “pure”)** with respect to the target variable.\n",
        "\n",
        "For example, if the target variable has two possible outcomes — say 1 and 0 (represented by green and red dots) — the Decision Tree looks for splits in the features that create groups mostly containing either 1’s or 0’s, rather than a mix of both.\n",
        "\n",
        ">In simple terms, the tree keeps splitting the data using feature values so that each group becomes as pure as possible in terms of the target outcome.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://towardsdatascience.com/wp-content/uploads/2022/11/1HQsjuYNRaphQ0SFXnedqRA.png\" alt=\"Pandas Illustration\" width=\"600\">\n",
        "</center>\n",
        "\n",
        "### **Definition**\n",
        "\n",
        "Entropy is defined (or calculated) as:\n",
        "\n",
        "$$\n",
        "Entropy(S) = - \\sum_{k=1}^{c} p_k \\log_2(p_k)\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- $S$ = the current node (or dataset)  \n",
        "- $c$ = number of classes  \n",
        "- $p_k$ = proportion of data points in class $k$ within node $S$  \n",
        "\n",
        "\n",
        "### **Properties of Entropy**\n",
        "\n",
        "- **Entropy = 0:**  \n",
        "  The node is **pure** — all samples belong to a single class (no uncertainty).  \n",
        "\n",
        "- **Entropy = 1:**  \n",
        "  The node is **maximally impure** — classes are **equally mixed** (e.g., 50% Pass, 50% Fail).  \n",
        "\n",
        "- **Range:**  \n",
        "  $ 0 \\leq Entropy(S) \\leq 1 $ for binary classification.  \n",
        "  (For multi-class problems, entropy can be higher, depending on the number of classes.)  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "If a node contains 4 samples:  \n",
        "- 3 are **Pass**,  \n",
        "- 1 is **Fail**,  \n",
        "\n",
        "then:\n",
        "\n",
        "$$\n",
        "Entropy(S) = -\\left( \\frac{3}{4} \\log_2 \\frac{3}{4} + \\frac{1}{4} \\log_2 \\frac{1}{4} \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "Entropy(S) = -(0.75 \\times -0.415) - (0.25 \\times -2)\n",
        "$$\n",
        "\n",
        "$$\n",
        "Entropy(S) \\approx 0.81\n",
        "$$\n",
        "\n",
        "So, this node has some impurity but is not completely mixed.\n",
        "\n",
        "### **Try it Yourself**\n",
        "\n",
        "If a node contains **5 examples**, where **4 are Pass** and **1 is Fail**, calculate the entropy.\n",
        "\n",
        "<!-- $$\n",
        "Entropy(S) = -\\left( \\frac{4}{5} \\log_2 \\frac{4}{5} + \\frac{1}{5} \\log_2 \\frac{1}{5} \\right)\n",
        "$$ -->\n",
        "\n",
        "You can use the Python function below to verify your answer!"
      ],
      "metadata": {
        "id": "6ptHP5Tqi-f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def entropy(class_counts):\n",
        "    \"\"\"\n",
        "    Compute entropy for a list of class counts.\n",
        "    class_counts: list of counts for each class\n",
        "    \"\"\"\n",
        "    total = sum(class_counts)\n",
        "    entropy_value = 0\n",
        "    for count in class_counts:\n",
        "        if count == 0:  # avoid log(0)\n",
        "            continue\n",
        "        p = count / total\n",
        "        entropy_value -= p * math.log2(p)\n",
        "    return entropy_value\n",
        "\n",
        "# Example 1: 4 Pass, 1 Fail\n",
        "print(\"Entropy (4 Pass, 1 Fail):\", round(entropy([4, 1]), 3))\n",
        "\n",
        "# Example 2: 2 Pass, 2 Fail (max impurity)\n",
        "print(\"Entropy (2 Pass, 2 Fail):\", round(entropy([2, 2]), 3))\n",
        "\n",
        "# Example 3: All Pass (pure node)\n",
        "print(\"Entropy (5 Pass, 0 Fail):\", round(entropy([5, 0]), 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnh7aYJWP7Ro",
        "outputId": "7dfb459c-930d-4721-c96c-78de757111bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy (4 Pass, 1 Fail): 0.722\n",
            "Entropy (2 Pass, 2 Fail): 1.0\n",
            "Entropy (5 Pass, 0 Fail): 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we know how to calculate **Entropy**, we can measure how much **uncertainty** (or impurity) is reduced when we split a dataset using a feature. This reduction is called **Information Gain**.\n",
        "\n",
        "## Information Gain (IG)\n",
        "Information Gain measures the **reduction in entropy** achieved by a split. It tells us **how much uncertainty (impurity) is removed** by a particular split.  \n",
        "\n",
        "### **Definition**\n",
        "\n",
        "Formally, suppose a split divides the parent node $S$ into $V$ subsets:  \n",
        "$$\n",
        "S_1, S_2, ..., S_V\n",
        "$$\n",
        "\n",
        "Let:  \n",
        "- $|S|$ = total number of samples in the parent node  \n",
        "- $|S_v|$ = number of samples in child node $S_v$  \n",
        "\n",
        "Information Gain for a feature **A** is defined as:\n",
        "\n",
        "$$\n",
        "IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\, Entropy(S_v)\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- $S$ = parent node (original dataset)  \n",
        "- $A$ = feature used for splitting  \n",
        "- $S_v$ = subset of $S$ corresponding to value $v$ of feature $A$  \n",
        "- $|S|$, $|S_v|$ = number of samples in $S$ and $S_v$, respectively  \n",
        "\n",
        "The algorithm selects the **feature with the highest Information Gain** for splitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **Intuition**\n",
        "\n",
        "Information Gain measures how much a split **reduces impurity**:\n",
        "- A **high IG** means the feature created **purer subsets** (good split).  \n",
        "- A **low IG** means the feature did not reduce impurity much (poor split).  \n",
        "\n",
        "Decision Trees choose the feature with the **highest Information Gain** at each step.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "Suppose a dataset has 10 samples:  \n",
        "- 6 **Pass**, 4 **Fail**  \n",
        "\n",
        "Then the initial (parent) entropy is:\n",
        "\n",
        "$$\n",
        "Entropy(S) = -\\left( \\frac{6}{10}\\log_2\\frac{6}{10} + \\frac{4}{10}\\log_2\\frac{4}{10} \\right) \\approx 0.971\n",
        "$$\n",
        "\n",
        "Now we split based on **Feature A** (e.g., “Study Hours”) which has two possible values:  \n",
        "- **A = High:** 5 samples → 4 Pass, 1 Fail  \n",
        "- **A = Low:** 5 samples → 2 Pass, 3 Fail  \n",
        "\n",
        "Compute entropy for each group:\n",
        "\n",
        "$$\n",
        "Entropy(S_{High}) = -\\left( \\frac{4}{5}\\log_2\\frac{4}{5} + \\frac{1}{5}\\log_2\\frac{1}{5} \\right) \\approx 0.722\n",
        "$$\n",
        "\n",
        "$$\n",
        "Entropy(S_{Low}) = -\\left( \\frac{2}{5}\\log_2\\frac{2}{5} + \\frac{3}{5}\\log_2\\frac{3}{5} \\right) \\approx 0.971\n",
        "$$\n",
        "\n",
        "Then the weighted average entropy after splitting is:\n",
        "\n",
        "$$\n",
        "Entropy_{after} = \\frac{5}{10}(0.722) + \\frac{5}{10}(0.971) = 0.847\n",
        "$$\n",
        "\n",
        "Finally, the **Information Gain** is:\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KjVvgo1Ks0WJb6i3tHwdBA.png\" alt=\"Pandas Illustration\" width=\"600\">\n",
        "</center>\n",
        "\n",
        "\n",
        "$$\n",
        "IG(S, A) = 0.971 - 0.847 = 0.124\n",
        "$$\n",
        "\n",
        "So, splitting on **Feature A** reduces uncertainty by **0.124 bits**.\n",
        "\n",
        "### **Additional Example:**\n",
        "Example illustrating the calculation of information gain. Source: Hendler 2018, slide 46.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://devopedia.org/images/article/168/4250.1555312917.jpg\" alt=\"Pandas Illustration\" width=\"600\">\n",
        "</center>\n",
        "\n",
        "---\n",
        "\n",
        "### **Try it Yourself**\n",
        "\n",
        "If you have another feature **B** that splits the same data into:\n",
        "- **B = Yes:** 6 samples (5 Pass, 1 Fail)  \n",
        "- **B = No:** 4 samples (1 Pass, 3 Fail)\n",
        "\n",
        "Try calculating:\n",
        "\n",
        "$$\n",
        "Entropy(S_{Yes}), \\; Entropy(S_{No}), \\; \\text{and} \\; IG(S, B)\n",
        "$$\n",
        "\n",
        "Which feature gives a higher Information Gain — **A** or **B**?\n"
      ],
      "metadata": {
        "id": "ZhVg7lEuQKNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def information_gain(parent_counts, split_counts_list):\n",
        "    \"\"\"\n",
        "    Compute Information Gain.\n",
        "    parent_counts: list of counts in the parent node\n",
        "    split_counts_list: list of lists, each sublist contains counts in a child node\n",
        "    \"\"\"\n",
        "    total_parent = sum(parent_counts)\n",
        "    parent_entropy = entropy(parent_counts)\n",
        "\n",
        "    weighted_entropy = 0\n",
        "    for counts in split_counts_list:\n",
        "        weight = sum(counts) / total_parent\n",
        "        weighted_entropy += weight * entropy(counts)\n",
        "\n",
        "    ig = parent_entropy - weighted_entropy\n",
        "    return ig\n",
        "\n",
        "# Example: Feature A split\n",
        "# Parent node: 6 Pass, 4 Fail\n",
        "parent = [6, 4]\n",
        "\n",
        "# Feature A splits:\n",
        "# High: 4 Pass, 1 Fail\n",
        "# Low: 2 Pass, 3 Fail\n",
        "split_A = [[4, 1], [2, 3]]\n",
        "\n",
        "print(\"Information Gain for Feature A:\", round(information_gain(parent, split_A), 3))\n",
        "\n",
        "# Feature B splits:\n",
        "# Yes: 5 Pass, 1 Fail\n",
        "# No: 1 Pass, 3 Fail\n",
        "split_B = [[5, 1], [1, 3]]\n",
        "\n",
        "print(\"Information Gain for Feature B:\", round(information_gain(parent, split_B), 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebMSkbc6V7_Z",
        "outputId": "f7a2d9d4-9248-43be-8b40-9dcc0a20f9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information Gain for Feature A: 0.125\n",
            "Information Gain for Feature B: 0.256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How Decision Trees Decide Which Feature to Split (Using Entropy and Information Gain)\n",
        "\n",
        "Decision Trees use **Entropy** and **Information Gain** to decide **how to split the data at each node**. The goal is to create **subgroups (child nodes) that are as pure as possible** with respect to the target variable.\n",
        "\n",
        "> The core idea of a Decision Tree is **recursive splitting**: at each node, we pick the **feature that best separates the data** into purer subgroups.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step-by-Step Process**\n",
        "\n",
        "1. **Start at the root node**  \n",
        "   - The root node contains the entire dataset.  \n",
        "   - Calculate the **Entropy** of the root node to measure the overall uncertainty.\n",
        "\n",
        "<!-- 2. **Evaluate all features for splitting**  \n",
        "   - For each feature, simulate splitting the data according to its possible values.  \n",
        "   - Compute the **weighted entropy** of the resulting child nodes.  \n",
        "   - Calculate the **Information Gain** for that feature:  \n",
        "$IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\, Entropy(S_v)$ -->\n",
        "\n",
        "2. **Evaluate each feature**  \n",
        "   - For each feature, split the dataset according to its possible values.  \n",
        "   - Compute the **entropy of each child node** and the **weighted average entropy**:\n",
        "$$\n",
        "Entropy_{after\\_split} = \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\, Entropy(S_v)\n",
        "$$\n",
        "\n",
        "   - Calculate **Information Gain**:\n",
        "\n",
        "$$\n",
        "IG(S, A) = Entropy(S) - Entropy_{after\\_split}\n",
        "$$\n",
        "\n",
        "   - A **higher IG** means the feature produces **purer child nodes**.\n",
        "\n",
        "3. **Choose the best feature to split**  \n",
        "   - Compare the Information Gain of all features.  \n",
        "   - Select the **feature with the highest IG** for splitting.  \n",
        "   - This ensures each split reduces the most uncertainty.\n",
        "      - Creates child nodes that are **more homogeneous** than the parent\n",
        "\n",
        "\n",
        "<!-- 3. **Choose the best feature to split**  \n",
        "   - The feature with the **highest Information Gain** is selected to split the current node.  \n",
        "   - This creates child nodes that are **more homogeneous** than the parent. -->\n",
        "\n",
        "\n",
        "4. **Repeat recursively for each child node**  \n",
        "   - Treat each child node as a new node and repeat Steps 1–3.  \n",
        "   - Continue until:\n",
        "     - Nodes are **pure** (entropy = 0), or  \n",
        "     - A **stopping condition** is reached (e.g., max depth, min samples per node).\n",
        "\n",
        "\n",
        "<!-- 4. **Repeat recursively for each child node**  \n",
        "   - For each child node, repeat the process: calculate entropy, evaluate features, and split using the feature with highest information gain.  \n",
        "   - Continue until:  \n",
        "     - Nodes are **pure** (all samples in the node belong to the same class), or  \n",
        "     - A **stopping criterion** is reached (e.g., max depth, min samples per node). -->\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Idea**\n",
        "\n",
        "- **Entropy** tells us how **mixed** the node is.  \n",
        "- **Information Gain** tells us how **good a feature is at reducing uncertainty**.  \n",
        "- Decision Trees **always pick the feature that maximizes Information Gain** to make the next split.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "mbCmWrjfZ5GT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detailed Example 01: How Decision Trees Decide Which Feature to Split\n",
        "\n",
        "Consider an example where we are building a decision tree to predict whether a loan given to a person would result in a write-off or not. Our entire population consists of 30 instances. 16 belong to the write-off class and the other 14 belong to the non-write-off class. We have two features, namely “Balance” that can take on two values -> “< 50K” or “>50K” and “Residence” that can take on three values -> “OWN”, “RENT” or “OTHER”.\n"
      ],
      "metadata": {
        "id": "2rOCHM8aogyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overfitting and Pruning\n",
        "\n",
        "## Overfitting in Decision Tree\n",
        "Decision trees are prone to overfitting, meaning they become too complex and capture noise in the training data (**perfectly fitting the training data**), but performing poorly on unseen data (**poor generalization**). This often happens with very deep trees with many nodes that achieve perfect classification on the training set (pure leaf nodes).\n",
        "\n",
        "## Pruning Methods\n",
        "\n",
        "**Pruning** is the technique used to combat overfitting by reducing the size of the tree. There are two main strategies:\n",
        "\n",
        "\n",
        "1.   **Pre-pruning (Early Stopping)** : Stop growing the tree early before it becomes too complex. How? by setting parameters like:\n",
        "\n",
        "*  Maximum depth of the tree.\n",
        "*  Minimum number of samples required to split a node.\n",
        "*  Minimum number of samples required to be in a leaf node.\n",
        "\n",
        "2.  **Post-pruning:** Grow a full tree and then remove branches that do not improve accuracy on validation data. The most common technique is Reduced Error Pruning.\n"
      ],
      "metadata": {
        "id": "CYecJO6Igfou"
      }
    }
  ]
}