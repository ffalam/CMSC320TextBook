{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmT0DkrTLJFl"
   },
   "source": [
    "# Section 2: The Tree-Building Algorithm: A Greedy, Recursive Process\n",
    "\n",
    "The construction of a Decision Tree is a **greedy**, **recursive**, and **top-down** process, known as Recursive Binary Splitting (or partitioning).\n",
    "\n",
    "\n",
    "> **Key Idea:** The core idea behind building a decision tree is **recursive partitioning**; repeatedly splitting the dataset based on an attribute that maximizes separation between classes (or minimizes error in regression).\n",
    "\n",
    "\n",
    "\n",
    "## Detailed Concept of Decision Tree Learning Algorithm:\n",
    "\n",
    "* **Greedy:** At each node, the algorithm selects the locally optimal split without considering the overall tree structure or future splits. It makes the best decision it can at that moment.\n",
    "\n",
    "* **Recursive:** The process is repeated for each subset of the data created by previous splits.\n",
    "\n",
    "* **Top-Down:** It begins at the root and successively splits the dataset into smaller groups.\n",
    "\n",
    "\n",
    "### The Core Algorithm (**ID3** and its successors **C4.5**, **CART**):\n",
    "\n",
    "The **Iterative Dichotomiser 3 (ID3)** algorithm is a decision tree induction algorithm that constructs decision trees using a top-down approach. The main goal of ID3 is to find the most informative attributes at each step to create partitions that yield the **highest information gain** or **decrease in impurity**.\n",
    "\n",
    "Let's break down the ID3 algorithm into a step-by-step process:\n",
    "\n",
    "- ➤ 1. **Start** at the root node with the entire dataset.\n",
    "\n",
    "- ➤ 2. **For each feature in the dataset**, calculate a *\"goodness\" metric* (e.g., Information Gain, Gini Impurity) for all possible split points.\n",
    "\n",
    "> In this chapter, we will focus on \"Entropy\" and \"Information Gain\".\n",
    "\n",
    "- ➤ 3. Select the feature and split point that **maximizes this \"goodness\" metric**. This becomes the splitting rule for the node.\n",
    "\n",
    "- ➤ 4. **Split the dataset at the current node** into child nodes based on the selected rule.\n",
    "\n",
    "The process recursively **repeat steps 2-4**S for each child node until **a stopping criterion** is met.\n",
    "\n",
    "**Common stopping criteria** include:\n",
    "\n",
    "- The node is **\"pure\"** (homogeneous: all samples belong to the same class).\n",
    "- No more attributes remain for further splitting.\n",
    "- A stopping condition (like a predefined maximum depth) is reached.\n",
    "- The node contains fewer samples than the predefined minimum threshold.\n",
    "- A split does not produce a significant improvement (minimum gain threshold)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ptHP5Tqi-f5"
   },
   "source": [
    "# Splitting Criteria (Attribute Selection Measures):\n",
    "\n",
    "The algorithm needs a metric to determine the **\"best\" split**. For classification trees, the goal is to **maximize the homogeneity** of the resulting subsets. The main metrics are **Entropy** and **Information Gain** (Used in ID3, C4.5).\n",
    "\n",
    "## Entropy\n",
    "\n",
    "Entropy measures the randomness or **impurity** in the dataset.\n",
    "\n",
    "\n",
    "\n",
    "> In information theory, the entropy of a random variable is the average level of \"information\", \"surprise\", or \"uncertainty\" inherent to the variable's possible outcomes.\n",
    "\n",
    "In a decision tree, a node that contains a mix of different outcomes (for example, 2 Pass and 2 Fail) has **higher entropy**, meaning more disorder or uncertainty. If a node has only one type of outcome (for example, all Pass or all Fail), it is **pure and has low entropy**.\n",
    "\n",
    "- **Maximum entropy = 1** → occurs when the outcomes are equally mixed (e.g., 50% Pass, 50% Fail).\n",
    "\n",
    "- **Minimum entropy = 0** → occurs when all outcomes are the same (e.g., 100% Pass or 100% Fail).\n",
    "\n",
    "> In short: Mixed outcomes → high entropy (uncertain) and Single outcome → low entropy (certain)\n",
    "\n",
    "\n",
    "<!-- <center>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1358/format:webp/1*E2qTQ1NSzy2Zb6YC8oqDvA.png\" alt=\"Pandas Illustration\" width=\"600\">\n",
    "</center> -->\n",
    "\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*ODTVm0g4PkSZnDMW91y_Qw.png\" alt=\"Pandas Illustration\" width=\"600\">\n",
    "</center>\n",
    "\n",
    "The first thing to understand about Decision Trees is that they split the data **based on the input features into smaller groups** that are **more homogeneous (or “pure”)** with respect to the target variable.\n",
    "\n",
    "For example, if the target variable has two possible outcomes — say 1 and 0 (represented by green and red dots) — the Decision Tree looks for splits in the features that create groups mostly containing either 1’s or 0’s, rather than a mix of both.\n",
    "\n",
    ">In simple terms, the tree keeps splitting the data using feature values so that each group becomes as pure as possible in terms of the target outcome.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://towardsdatascience.com/wp-content/uploads/2022/11/1HQsjuYNRaphQ0SFXnedqRA.png\" alt=\"Pandas Illustration\" width=\"600\">\n",
    "</center>\n",
    "\n",
    "### **Definition**\n",
    "\n",
    "Entropy is defined (or calculated) as:\n",
    "\n",
    "$$\n",
    "Entropy(S) = - \\sum_{k=1}^{c} p_k \\log_2(p_k)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $S$ = the current node (or dataset)  \n",
    "- $c$ = number of classes  \n",
    "- $p_k$ = proportion of data points in class $k$ within node $S$  \n",
    "\n",
    "\n",
    "### **Properties of Entropy**\n",
    "\n",
    "- **Entropy = 0:**  \n",
    "  The node is **pure** — all samples belong to a single class (no uncertainty).  \n",
    "\n",
    "- **Entropy = 1:**  \n",
    "  The node is **maximally impure** — classes are **equally mixed** (e.g., 50% Pass, 50% Fail).  \n",
    "\n",
    "- **Range:**  \n",
    "  $ 0 \\leq Entropy(S) \\leq 1 $ for binary classification.  \n",
    "  (For multi-class problems, entropy can be higher, depending on the number of classes.)  \n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "\n",
    "If a node contains 4 samples:  \n",
    "- 3 are **Pass**,  \n",
    "- 1 is **Fail**,  \n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "Entropy(S) = -\\left( \\frac{3}{4} \\log_2 \\frac{3}{4} + \\frac{1}{4} \\log_2 \\frac{1}{4} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Entropy(S) = -(0.75 \\times -0.415) - (0.25 \\times -2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Entropy(S) \\approx 0.81\n",
    "$$\n",
    "\n",
    "So, this node has some impurity but is not completely mixed.\n",
    "\n",
    "### **Try it Yourself**\n",
    "\n",
    "If a node contains **5 examples**, where **4 are Pass** and **1 is Fail**, calculate the entropy.\n",
    "\n",
    "<!-- $$\n",
    "Entropy(S) = -\\left( \\frac{4}{5} \\log_2 \\frac{4}{5} + \\frac{1}{5} \\log_2 \\frac{1}{5} \\right)\n",
    "$$ -->\n",
    "\n",
    "You can use the Python function below to verify your answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def entropy(class_counts):\n",
    "    \"\"\"\n",
    "    Compute entropy for a list of class counts.\n",
    "    class_counts: list of counts for each class\n",
    "    \"\"\"\n",
    "    total = sum(class_counts)\n",
    "    entropy_value = 0\n",
    "    for count in class_counts:\n",
    "        if count == 0:  # avoid log(0)\n",
    "            continue\n",
    "        p = count / total\n",
    "        entropy_value -= p * math.log2(p)\n",
    "    return entropy_value\n",
    "\n",
    "# Example 1: 4 Pass, 1 Fail\n",
    "print(\"Entropy (4 Pass, 1 Fail):\", round(entropy([4, 1]), 3))\n",
    "\n",
    "# Example 2: 2 Pass, 2 Fail (max impurity)\n",
    "print(\"Entropy (2 Pass, 2 Fail):\", round(entropy([2, 2]), 3))\n",
    "\n",
    "# Example 3: All Pass (pure node)\n",
    "print(\"Entropy (5 Pass, 0 Fail):\", round(entropy([5, 0]), 3))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
