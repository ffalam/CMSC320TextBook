{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "##  Distance Metrics and Variants in KNN\n",
    "\n",
    "The **choice of distance metric** greatly influences how KNN defines \u201cneighborhoods\u201d and classifies points.  \n",
    "It depends on the **nature of the data** and the **problem context**.\n",
    "\n",
    "### Common Distance Metrics\n",
    "\n",
    "<!-- | Distance Type | Description | Best Used When |\n",
    "|----------------|----------------------|----------------|\n",
    "| **Euclidean Distance** | Straight-line distance between two points.  | Continuous numeric features (default in most KNN implementations). |\n",
    "| **Manhattan Distance** | Sum of absolute differences between coordinates.  | When movement is restricted to grid-like paths (e.g., city blocks). |\n",
    "| **Hamming Distance** | Counts number of feature mismatches.  | For **categorical or binary** features (e.g., \u201cyes/no\u201d). |\n",
    "| **Cosine Similarity** | Measures the cosine of the angle between two vectors. | When **magnitude doesn\u2019t matter**, such as text or document embeddings. | -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- Let **x** and **y** be two points in *n*-dimensional space.\n",
    "\n",
    "\n",
    "| Distance Type | Formula |  |\n",
    "| :--- | :--- | :--- |\n",
    "| **Euclidean Distance** | $$d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$$ |  |\n",
    "| **Manhattan Distance** | $$d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^{n} |x_i - y_i|$$ |  |\n",
    "| **Hamming Distance** | $$d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^{n} \\mathbb{I}(x_i \\neq y_i)$$ |  |\n",
    "| **Cosine Similarity** | $$\\text{Similarity}(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|}$$ |  |\n",
    "\n",
    "\n",
    " -->\n",
    "\n",
    "<!--\n",
    "### How to Choose the Distance Metric\n",
    "- **Euclidean** \u2192 When scale and magnitude are meaningful.  \n",
    "- **Manhattan** \u2192 When features represent *independent additive effects*.  \n",
    "- **Hamming** \u2192 For categorical or yes/no variables.  \n",
    "- **Cosine** \u2192 For angle-based comparisons, e.g., text or high-dimensional sparse data. -->\n",
    "\n",
    "## 1. Euclidean Distance (L2 Norm)\n",
    "\n",
    "| Detail | Description |\n",
    "| :--- | :--- |\n",
    "| **Description** | Measures the **straight-line distance** between two points in space. |\n",
    "| **Use When** | Features are **continuous** and measured on the **same scale**. Differences in magnitude are meaningful (e.g., height, weight). |\n",
    "| **Caution** | **Highly sensitive to outliers** and varying feature scales \u2014 **always normalize or standardize** data before use. |\n",
    "| **Formula** | $$d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$$ |\n",
    "\n",
    "## 2. Manhattan (City Block) Distance (L1 Norm)\n",
    "\n",
    "| Detail | Description |\n",
    "| :--- | :--- |\n",
    "| **Description** | Measures the distance by **summing absolute differences** along each dimension (like moving along a grid). |\n",
    "| **Use When** | Movement is restricted to **orthogonal directions** (e.g., city blocks). Features contribute independently to distance. Works better than Euclidean for **high-dimensional data** as it reduces the impact of large single-feature deviations. |\n",
    "| **Caution** | Less sensitive to outliers than Euclidean, but still requires feature scaling for optimal performance. |\n",
    "| **Formula** | $$d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^{n} |x_i - y_i|$$ |\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/v2/1*nx_7Z__Nnx05PA-jpGEcxw.jpeg\n",
    "\" alt=\"Pandas Illustration\" width=\"600\">\n",
    "</center>\n",
    "\n",
    "## 3. Hamming Distance\n",
    "\n",
    "| Detail | Description |\n",
    "| :--- | :--- |\n",
    "| **Description** | Counts the **number of features where two samples differ** (mismatches between attribute values). |\n",
    "| **Use When** | Data is **categorical or binary** (e.g., yes/no, text encodings, DNA sequences). |\n",
    "| **Caution** | **Only works for discrete** or categorical features. It gives a binary measure (different or not different) for each feature. |\n",
    "| **Formula** | $$d(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^{n} \\mathbb{I}(x_i \\neq y_i)$$ (Where $\\mathbb{I}(\\cdot)$ is the indicator function.) |\n",
    "\n",
    "## 4. Cosine Similarity\n",
    "\n",
    "| Detail | Description |\n",
    "| :--- | :--- |\n",
    "| **Description** | Measures the **cosine of the angle** between two vectors \u2014 focuses purely on **orientation**, not magnitude or size. |\n",
    "| **Use When** | **Direction matters more than size** (e.g., text analysis, where document length shouldn't bias similarity). Common in **recommender systems**. |\n",
    "| **Caution** | Returns **similarity** (1 is close, 0 is far) rather than distance. It is often converted to a distance metric as: **$d = 1 - \\text{similarity}$**. |\n",
    "| **Formula** | $$\\text{Similarity}(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|}$$ |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Weighted KNN\n",
    "\n",
    "So far, we have talked about **standard KNN**, where every one of the k neighbors has an equal vote. Now, we will introduce **Weighted K-Nearest Neighbors** changes this by giving more weight to the neighbors that are closer to the new data point.\n",
    "\n",
    "Why? According to **Weighted KNN**, closer neighbors have a stronger influence on the prediction.\n",
    "\n",
    "Mathematically, each neighbor is weighted by:\n",
    "\n",
    "$$\n",
    "w_i = \\frac{1}{d_i + \\varepsilon}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "d_i \\text{ = distance between the query point and the } i^{\\text{th}} \\text{ neighbor}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\varepsilon \\text{ = a small constant to prevent division by zero}\n",
    "$$\n",
    "\n",
    "This means points closer to the query point contribute **more** to the prediction than distant ones.\n",
    "\n",
    "\n",
    "In `scikit-learn`, you can enable this using:\n",
    "```python\n",
    "KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "```\n",
    "\n",
    "**Key Points to remember:**\n",
    "\n",
    "- Standard KNN treats all neighbors equally (weights='uniform').\n",
    "- Weighted KNN emphasizes closer neighbors, making the classifier more robust to noise.\n",
    "- Works best when your features are continuous numeric values (like Iris measurements).-\n"
   ],
   "metadata": {
    "id": "scQQdg5wT3QQ"
   },
   "id": "scQQdg5wT3QQ"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Section 6: Ethics and Responsible ML\n",
    "\n",
    "### Why Ethics Matter\n",
    "Machine Learning models can impact real people \u2014 from job applications to healthcare decisions. Therefore, fairness and transparency are essential.\n",
    "\n",
    "- **Bias:** When data reflects historical prejudice.\n",
    "- **Fairness:** Ensure models treat groups equally.\n",
    "- **Transparency:** Explain how the model makes decisions.\n",
    "- **Privacy:** Respect individuals\u2019 personal data.\n",
    "\n",
    "> Example: A hiring model should not unfairly prefer one gender or race.\n",
    "\n"
   ],
   "metadata": {
    "id": "iSG4i-7VJ_o7"
   },
   "id": "iSG4i-7VJ_o7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Section 7: Hands-on Practice\n",
    "\n",
    "### Task: Identify the ML Type\n",
    "1. Predicting house prices \u2192 ?\n",
    "2. Grouping customers \u2192 ?\n",
    "3. Teaching a robot to play chess \u2192 ?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "sIcg10lVKCxQ"
   },
   "id": "sIcg10lVKCxQ"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Section 8: Reflection Questions\n",
    "1. What distinguishes Machine Learning from traditional programming?\n",
    "2. Why is testing important after training a model?\n",
    "3. How can bias in ML models be reduced?\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "19WRtYXCKFYc"
   },
   "id": "19WRtYXCKFYc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### Summary: What We Learned\n",
    "\n",
    "In this chapter, we walked through the **complete Machine Learning workflow** \u2014 from data to deployment-ready model (conceptually).\n",
    "\n",
    "**We covered:**\n",
    "1. **Understanding the dataset** \u2013 explored real data (Iris flower dataset).  \n",
    "2. **Defining features and labels** \u2013 identified what to predict and what to use for prediction.  \n",
    "3. **Training a model** \u2013 taught a KNN to learn from data.  \n",
    "4. **Evaluating performance** \u2013 measured accuracy on unseen test data.  \n",
    "5. **Visualizing data** \u2013 saw how features relate and form clusters.\n",
    "\n",
    "**Key takeaway:**  \n",
    "Machine Learning is about teaching computers to **learn from data and improve over time** \u2014 not just follow fixed rules.\n",
    "- ML enables systems to learn from data automatically.\n",
    "- Three main types: Supervised, Unsupervised, and Reinforcement.\n",
    "- Workflow: Data \u2192 Model \u2192 Evaluation \u2192 Deployment.\n",
    "- Ethical ML ensures fairness, transparency, and accountability.\n",
    "\n",
    "\n",
    "## Knowledge Check\n",
    "\n",
    "<iframe\n",
    "src=\"https://docs.google.com/forms/d/e/1FAIpQLSdHPGUAqVTvyLe4jkZee7qwZkZU5S9Ma_yzU_ISc2htFW3RnQ/viewform?embedded=true\"\n",
    "width=\"100%\"\n",
    "  height=\"800px\"\n",
    "  frameborder=\"0\"\n",
    "  style=\"min-height: 800px; height: 100vh\"\n",
    ">Loading\u2026</iframe>\n"
   ],
   "metadata": {
    "id": "RSjPpQVKLTxZ"
   },
   "id": "RSjPpQVKLTxZ"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}