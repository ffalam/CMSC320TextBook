{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8USDyixAhtY"
      },
      "source": [
        "## **18.5 Loss Functions**\n",
        "Loss functions (or cost functions) measure how well a neural network’s predictions match the true target values. During training, the goal is to minimize the loss by adjusting the model’s parameters. The choice of loss function depends on the type of task:\n",
        "\n",
        "\n",
        "\n",
        " - **Mean Squared Error (MSE)**: is widely used in regression problems, such as predicting house prices or temperature, where the  output is continuous and goal is to minimize the average squared difference between predicted and actual values.$$L_{MSE} = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        " As it calculates the average squared difference between predicted values and actual values, which means larger errors are penalized more heavily. While MSE is straightforward and differentiable, making it compatible with gradient descent, it has notable drawbacks: it is highly sensitive to outliers due to the squaring operation, and it performs poorly in classification tasks, often leading to slow convergence.\n",
        "\n",
        " - **Cross-Entropy Loss**: On the other hand, Cross-Entropy Loss is widely used for classification tasks, both binary and multi-class. It measures the difference between the predicted probability distribution and the true label distribution.\n",
        "- For binary classification:\n",
        "\n",
        "$$L_{BCE} = -\\frac{1}{N}\\sum_{i=1}^N \\left[y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)\\right]$$\n",
        "-   - For multi-class classification:\n",
        "\n",
        "$$L_{CCE} = -\\frac{1}{N}\\sum_{i=1}^N \\sum_{c=1}^C y_{i,c}\\log(\\hat{y}_{i,c})$$\n",
        "\n",
        " For binary classification, it penalizes the model when it confidently predicts the wrong class, encouraging outputs closer to the true labels. In multi-class settings, cross-entropy works with softmax outputs to handle multiple classes simultaneously. One disadvantage of cross-entropy loss is that it can become very large when the model assigns near-zero probabilities to the true class, which may cause instability during training if not handled properly.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
