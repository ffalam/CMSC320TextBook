{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whuZyM0rAzN9"
      },
      "source": [
        "## **18.6 Optimizers**\n",
        "\n",
        "Optimizers are algorithms that adjust the weights of a neural network to minimize the loss function during training. Two most basic optimizers are\n",
        " - **SGD (Stochastic Gradient Descent)**: Basic optimization\n",
        " - **Adam**: Adaptive learning rates (most popular)\n",
        "\n",
        "Stochastic Gradient Descent (SGD) updates the modelâ€™s parameters by computing the gradient of the loss on a small batch of data and moving in the direction that reduces the loss. While simple and effective, SGD uses a fixed learning rate and can be slow to converge, especially for complex models.\n",
        "\n",
        "To address these limitations, Adam (Adaptive Moment Estimation) is widely used due to its ability to adapt the learning rate for each parameter individually by combining the benefits of momentum and RMSProp optimizers. Adam often results in faster convergence and better performance without much tuning, making it the most popular choice for training deep neural networks.\n",
        "\n",
        "<sub>Note: Momentum helps speed up learning by smoothing updates using past gradients, while RMSProp adapts learning rates based on recent gradient magnitudes. Adam combines both techniques for efficient and stable training.</sub>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
