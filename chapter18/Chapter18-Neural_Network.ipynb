{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 18: Introduction to the  Neural Network\n",
        "\n",
        "\n",
        "Artificial intelligence has revolutionized the way machines learn from data, enabling them to perform tasks that once seemed impossible. From powering voice assistants to enabling autonomous vehicles, AI systems are now integrated into many aspects of daily life. At the core of this transformation are neural networks—computational models inspired by **the structure and function of the human brain**. These networks consist of interconnected nodes, or \"neurons,\" that process information in layers, allowing them to recognize patterns, make decisions, and improve over time. From early single-layer perceptrons to today’s deep architectures, neural networks have become a cornerstone of modern AI, powering breakthroughs in fields like healthcare, finance, and autonomous systems.\n",
        "\n",
        "> Neural networks are a **type of supervised learning algorithm** capable of identifying intricate patterns and relationships within data, making them suitable for tackling problems that traditional models often struggle with.\n",
        "\n",
        "![NN](NN.png)\n",
        "\n",
        "\n",
        "Although neural network algorithms have existed for many years, recent advancements in their architectures have led to significant improvements in performance on large-scale machine learning tasks. These developments form the foundation of what is now referred to as the \"deep learning\" methodology.\n",
        "\n",
        "Deep learning, a branch of machine learning and artificial intelligence, leverages neural networks with multiple hidden layers to address highly complex tasks. These tasks range from natural language processing—such as speech recognition and text interpretation—to computer vision applications like object detection and image classification. The rise of deep learning over the past twenty years can be attributed to its remarkable effectiveness, the surge in computational power, and the growing accessibility of vast datasets."
      ],
      "metadata": {
        "id": "vG0Z1u8yTPRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 18.1 Fundamentals of Neural Networks\n",
        "\n",
        "Neural networks are a powerful class of supervised learning algorithms capable of modeling complex, nonlinear relationships in data. Unlike traditional machine learning models, which rely on handcrafted features, neural networks automatically learn hierarchical representations from raw input. Key components include:\n",
        "\n",
        "* **Input Layer:** Receives raw data (e.g., pixels in an image, words in a text).\n",
        "\n",
        "* **Hidden Layers:** Intermediate layers that transform inputs through weighted connections and activation functions.\n",
        "\n",
        "* **Output Layer:** Produces the final prediction (e.g., class label, regression value).\n",
        "\n",
        "* **Weights & Biases:** Adjustable parameters learned during training.\n",
        "\n",
        "* **Activation Functions:** Introduce nonlinearity (e.g., ReLU, Sigmoid, Tanh).\n",
        "\n",
        "COMMENT: WE CAN ADD SOME GIF VISUALIZATION of Neural Network HERE: input later to hidden output\n",
        "\n",
        "Training a neural network involves **forward propagation** (passing data through the network) and **backpropagation **(adjusting weights based on error gradients using optimization techniques like gradient descent)."
      ],
      "metadata": {
        "id": "Z2Ta9SqhT8lu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **18.1 Neurons: The Building Blocks**\n",
        "\n",
        "A neuron (or node) is the fundamental unit of a neural network, mimicking biological neurons. It receives inputs, processes them using weights and an activation function, and produces an output. Mathematically, a neuron's operation can be represented as:\n",
        "\n",
        "\n",
        "$$ y = f\\left(\\sum_{i=1}^{n} (w_i x_i) + b\\right) $$\n",
        "\n",
        "Where:\n",
        "- $x_i$ = input features\n",
        "- $w_i$ = weights\n",
        "- $b$ = bias term  \n",
        "- $f$ = activation function\n",
        "\n",
        "\n",
        "## **4.2.2 Layers: Input, Hidden, and Output**\n",
        "Neural networks are organized into layers:\n",
        "\n",
        "1. **Input Layer**:\n",
        "    - The first layer that receives raw data (e.g., pixel values in an image, word embeddings in text).\n",
        "\n",
        "2. **Hidden Layers**:\n",
        "    - Intermediate layers between input and output where feature extraction and transformation occur.\n",
        "    - Deep networks have multiple hidden layers\n",
        "\n",
        " 3. **Output Layer**:\n",
        "    - Produces final predictions\n",
        "    - Classification: class probabilities\n",
        "    - Regression: continuous values\n",
        "\n",
        "\n",
        "## **18.3 Weights and Biases**\n",
        " | Component | Role |\n",
        " |-----------|------|\n",
        " | **Weights** (\\( w_i \\)) | Determine connection strength between neurons. They are adjusted during training to minimize prediction errors. |\n",
        "  | **Bias** (\\( b \\)) | Allows shifting the activation function to improve model flexibility. |\n",
        "\n",
        "\n",
        "\n",
        "## **18.4 Activation Functions**\n",
        "\n",
        "Activation functions are essential components of neural networks, introducing non-linearities that enable the model to learn complex patterns. Without them, a neural network would simply be a linear regression model, incapable of handling intricate data relationships. This section explores four widely used activation functions: Sigmoid, ReLU, Tanh, and Softmax, discussing their properties, advantages, and limitations. Each has unique properties and use cases depending on the architecture and goal of the model.\n",
        "\n",
        "\n",
        "| Function  | Formula                          | Use Case                      |\n",
        "|-----------|----------------------------------|-------------------------------|\n",
        "| **Sigmoid** | $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ | Binary classification  (outputs between 0 and 1)         |\n",
        "| **ReLU**   | $f(x) = \\max(0, x)$              | Default choice for hidden layers (fast computation)    |\n",
        "| **Tanh**   | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | Similar to sigmoid but outputs between -1 and 1 |\n",
        "| **Softmax**| $\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$ | Multi-class classification (outputs probabilities) |\n",
        "\n",
        "\n",
        "**The sigmoid function:**  maps any real-valued number to a range between 0 and 1, making it suitable for binary classification tasks where outputs represent probabilities.\n",
        "This function $\\sigma(x) = \\frac{1}{1 + e^{-x}}$  maps any real-valued input into a probability-like output, making it useful in binary classification and output layers where probabilistic interpretation is needed.\n",
        "\n",
        "Advantages:\n",
        "* Useful when outputs need to be interpreted as probabilities.\n",
        "*Differentiable, allowing gradient-based optimization.\n",
        "\n",
        "\n",
        "Disadvantages:\n",
        "* Vanishing Gradients: For very large or small inputs, gradients become nearly zero, slowing down learning.\n",
        "* Not Zero-Centered: Outputs are always positive, leading to inefficient weight updates\n",
        "* Computationally Expensive: Involves exponentiation operations.\n",
        "\n",
        "\n",
        "**ReLU (Rectified Linear Unit) Function:** is one of the most popular activation functions due to its simplicity and effectiveness. It outputs the input directly if positive; otherwise, it outputs zero ($f(x) = \\max(0, x)$). It is non-linear but simple; computationally efficient.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "* Avoids Vanishing Gradient (for positive inputs): Unlike sigmoid, gradients remain strong for active neurons.\n",
        "* Fast Computation: No complex exponentials.\n",
        "* Sparsity: Can deactivate neurons (output zero), making the network more efficient.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "* Dying ReLU Problem: If many neurons output zero (due to negative inputs), they stop learning entirely.\n",
        "*\n",
        "Not Zero-Centered: Like sigmoid, can lead to slower convergence.\n",
        "\n",
        "What is the **Dying ReLU** Problem?\n",
        "If a neuron consistently receives negative inputs, its output becomes zero, and its weights stop updating (since the gradient is also zero). Over time, this can cause some neurons to \"die\" and never activate again, reducing the model’s capacity to learn (neurons stop contributing to learning).\n",
        "\n",
        "Solutions to Dying ReLU:\n",
        "\n",
        "* Leaky ReLU: Allows a small negative slope (e.g., 0.01) for negative inputs.\n",
        "$$\n",
        "\\text{Leaky ReLU}(x) =\n",
        "\\begin{cases}\n",
        "x, & \\text{if } x \\geq 0 \\\\\n",
        "\\alpha x, & \\text{if } x < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is a small positive constant (e.g., 0.01).\n",
        "\n",
        "* Parametric ReLU (PReLU): Learns the negative slope during training.\n",
        "\n",
        "* Exponential Linear Unit (ELU): Smoothly handles negative inputs.\n",
        "\n",
        "**Tanh (Hyperbolic Tangent) Function:**\n",
        "The tanh function is similar to sigmoid but maps inputs to a range between -1 and 1, making it zero-centered, $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$.\n",
        "* In neural networks, an activation function is zero-centered if its output values are symmetrically distributed around zero (i.e., they have a mean of zero). This property helps in maintaining stable and efficient training by preventing systematic weight updates in a single direction.\n",
        "\n",
        "* When activation outputs are not zero-centered (e.g., sigmoid outputs between 0 and 1), gradients during backpropagation tend to be either all positive or all negative, leading to inefficient weight updates: tend to update in the same direction (either always increasing or always decreasing), slowing down convergence.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "* Zero-centered output allows for better convergence during gradient descent.\n",
        "\n",
        "* Stronger gradients than sigmoid for inputs near 0.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "* Still suffers from the vanishing gradient problem for very large or very small inputs:: Like sigmoid, gradients become very small for extreme values.\n",
        "\n",
        "* Slightly More Computationally Expensive: Due to exponential operations.\n",
        "\n",
        "**The Softmax function** is typically used in the output layer of a multi-class classification model. It converts a vector of raw scores (logits) into a probability distribution over predicted output classes: **Softmax**| $\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$.\n",
        "\n",
        "\n",
        "Advantages:\n",
        "\n",
        "* Ensures that the sum of the outputs is 1, making them interpretable as probabilities.\n",
        "\n",
        "* Highlights the highest-valued input while suppressing the rest, which helps in clear class predictions.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "* Exponentially sensitive to input scale—can cause numerical instability if logits are too large.\n",
        "\n",
        "* When classes are not mutually exclusive, Softmax is not ideal (use sigmoid instead for multi-label classification).\n",
        "\n",
        "\n",
        "\n",
        "## **4.2.5 Loss Functions**\n",
        "Loss functions (or cost functions) measure how well a neural network’s predictions match the true target values. During training, the goal is to minimize the loss by adjusting the model’s parameters. The choice of loss function depends on the type of task:\n",
        "\n",
        "\n",
        "\n",
        " - **Mean Squared Error (MSE)**: is widely used in regression problems, such as predicting house prices or temperature, where the  output is continuous and goal is to minimize the average squared difference between predicted and actual values.$$L_{MSE} = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        " As it calculates the average squared difference between predicted values and actual values, which means larger errors are penalized more heavily. While MSE is straightforward and differentiable, making it compatible with gradient descent, it has notable drawbacks: it is highly sensitive to outliers due to the squaring operation, and it performs poorly in classification tasks, often leading to slow convergence.\n",
        "\n",
        " - **Cross-Entropy Loss**: On the other hand, Cross-Entropy Loss is widely used for classification tasks, both binary and multi-class. It measures the difference between the predicted probability distribution and the true label distribution.\n",
        "- For binary classification:\n",
        "\n",
        "$$L_{BCE} = -\\frac{1}{N}\\sum_{i=1}^N \\left[y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)\\right]$$\n",
        "-   - For multi-class classification:\n",
        "\n",
        "$$L_{CCE} = -\\frac{1}{N}\\sum_{i=1}^N \\sum_{c=1}^C y_{i,c}\\log(\\hat{y}_{i,c})$$\n",
        "\n",
        " For binary classification, it penalizes the model when it confidently predicts the wrong class, encouraging outputs closer to the true labels. In multi-class settings, cross-entropy works with softmax outputs to handle multiple classes simultaneously. One disadvantage of cross-entropy loss is that it can become very large when the model assigns near-zero probabilities to the true class, which may cause instability during training if not handled properly.\n",
        "\n",
        "\n",
        "## **4.2.6 Optimizers**\n",
        "\n",
        "Optimizers are algorithms that adjust the weights of a neural network to minimize the loss function during training. Two most basic optimizers are\n",
        " - **SGD (Stochastic Gradient Descent)**: Basic optimization\n",
        " - **Adam**: Adaptive learning rates (most popular)\n",
        "\n",
        "Stochastic Gradient Descent (SGD) updates the model’s parameters by computing the gradient of the loss on a small batch of data and moving in the direction that reduces the loss. While simple and effective, SGD uses a fixed learning rate and can be slow to converge, especially for complex models.\n",
        "\n",
        "To address these limitations, Adam (Adaptive Moment Estimation) is widely used due to its ability to adapt the learning rate for each parameter individually by combining the benefits of momentum and RMSProp optimizers. Adam often results in faster convergence and better performance without much tuning, making it the most popular choice for training deep neural networks.\n",
        "\n",
        "<sub>Note: Momentum helps speed up learning by smoothing updates using past gradients, while RMSProp adapts learning rates based on recent gradient magnitudes. Adam combines both techniques for efficient and stable training.</sub>\n",
        "\n",
        "## 4.2.7 Training Techniques & Regularization\n",
        "\n",
        "Training a neural network consists of two main steps performed repeatedly over many iterations: the **forward pass** and the **backward pass**.\n",
        "\n",
        "### Forward Pass\n",
        "\n",
        "During the forward pass, input data flows through the network, and the final output layer generates predictions $(\\hat{y}^{(i)})$ for each input example \\(i\\). The network then computes the loss function \\(L\\), which quantifies the difference between predicted outputs and true targets  $(y^{(i)}\\)$. For a batch of \\(m\\) samples, the average loss is:\n",
        "\n",
        "$$\n",
        "L = \\frac{1}{m} \\sum_{i=1}^m L\\left(\\hat{y}^{(i)}, y^{(i)}\\right)\n",
        "$$\n",
        "\n",
        "This loss guides how well the model is performing.\n",
        "\n",
        "### Backward Pass (Backpropagation)\n",
        "\n",
        "In the backward pass, gradients of the loss with respect to each parameter (weights \\(W^{[l]}\\) at layer \\(l\\)) are computed using the **chain rule**:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W^{[l]}} = \\frac{\\partial L}{\\partial a^{[l]}} \\cdot \\frac{\\partial a^{[l]}}{\\partial z^{[l]}} \\cdot \\frac{\\partial z^{[l]}}{\\partial W^{[l]}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\(a^{[l]}\\) is the activation output of layer \\(l\\),\n",
        "- \\(z^{[l]}\\) is the linear combination \\(W^{[l]} a^{[l-1]} + b^{[l]}\\).\n",
        "\n",
        "Parameters are updated using gradient descent with learning rate \\(\\alpha\\):\n",
        "\n",
        "$$\n",
        "W^{[l]} := W^{[l]} - \\alpha \\frac{\\partial L}{\\partial W^{[l]}}\n",
        "$$\n",
        "\n",
        "### Regularization Techniques\n",
        "\n",
        "Regularization adds penalty terms to the loss function to prevent overfitting by constraining model complexity:\n",
        "\n",
        "- **L2 Regularization (Weight Decay):**\n",
        "\n",
        "$$\n",
        "L_{\\text{reg}} = L + \\lambda \\sum \\|W\\|_2^2\n",
        "$$\n",
        "\n",
        "- **L1 Regularization:**\n",
        "\n",
        "$$\n",
        "L_{\\text{reg}} = L + \\lambda \\sum |W|\n",
        "$$\n",
        "\n",
        "Where \\(\\lambda\\) controls the regularization strength.\n",
        "\n",
        "Adding these terms encourages smaller or sparser weights, improving generalization on unseen data.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kn4cLQMHvj3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference:"
      ],
      "metadata": {
        "id": "srJwvzKYvDCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.89879&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
      ],
      "metadata": {
        "id": "d-G3nnlhSavw"
      }
    }
  ]
}