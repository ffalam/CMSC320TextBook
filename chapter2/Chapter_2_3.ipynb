{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYjKwdxIERPo"
   },
   "source": [
    "# Where Does Data Come From?\n",
    "\n",
    "Historically, datasets arrived through organized channels: government surveys, institutional studies, corporate databases. Today, data spills from everyday systems. Almost every action, transaction, or device leaves a trace.\n",
    "\n",
    "Common sources include:\n",
    "\n",
    "- transactional databases\n",
    "- sensors and wearables\n",
    "- web APIs\n",
    "- web scraping\n",
    "- user interactions\n",
    "- application logs\n",
    "\n",
    "Some sources deliver structured data; others arrive messy and require transformation before analysis.\n",
    "\n",
    "## **Scraping vs. APIs**\n",
    "\n",
    "When acquiring data from the web, two patterns are common: **web scraping** and **APIs**.\n",
    "\n",
    "**Web scraping** imitates a browser, reading HTML and extracting what a human would see. It is useful when no official data interface exists, but fragile and sensitive to ethical and legal boundaries.\n",
    "\n",
    "**APIs**, in contrast, are explicit contracts:\n",
    "\n",
    "> “If you send a structured request, I will send structured information.”\n",
    "\n",
    "APIs typically return machine-readable data (often JSON), avoiding the need to parse HTML manually.\n",
    "\n",
    "Before scraping, it is ***always worth checking***  if an API exists.\n",
    "\n",
    "\n",
    "### **How Web Scraping Works (Conceptual)**\n",
    "\n",
    "<center>\n",
    "  <img src=\"https://monashdatafluency.github.io/python-web-scraping/images/request.png\" width=\"400\">\n",
    "</center>\n",
    "\n",
    "1. Your script sends an HTTP GET request to a URL.  \n",
    "2. The server returns an HTML page.  \n",
    "3. Your program parses and extracts the relevant parts.\n",
    "\n",
    "\n",
    "\n",
    "### **(a) Beautiful Soup and Parsing HTML**\n",
    "\n",
    "When websites lack an API, we fall back to HTML, the layer designed for humans. **Beautiful Soup** converts that visual layer into structured data.\n",
    "\n",
    "> **BeautifulSoup** is a Python library for parsing and extracting information from HTML; it takes the raw HTML returned by a website and lets you navigate and extract useful parts such as tables, links, or text.\n",
    "\n",
    "HTML is hierarchical: tags contain tags, attributes provide meaning, and text sits inside. Beautiful Soup lets us locate elements (`div`, `table`, `span`) and extract text, links, or tabular content.\n",
    "\n",
    "<center>\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*6UIaApn54TOkhOQ607z-cw.jpeg\" width=\"420\">\n",
    "  <img src=\"https://2.bp.blogspot.com/-oeOzu13C26U/V1_2uXbFE4I/AAAAAAAAALI/2RmiWjCb--YUVO6MAg3pG5eIOISFkVwBgCLcB/s1600/custom-web-scraping-624x301.png\" width=\"420\">\n",
    "  <img src=\"https://monashdatafluency.github.io/python-web-scraping/section-2-HTML-based-scraping/figures/html_structure.png\" width=\"420\">\n",
    "  <br>\n",
    "  <em><strong>Figure X.</strong> HTML-based scraping workflow: identify elements visually, locate tags, extract content, convert to structured data.</em>\n",
    "</center>\n",
    "\n",
    "Scraping is useful for product data, weather reports, job postings, and research tables — information that exists publicly but without a formal data interface.\n",
    "\n",
    "\n",
    "\n",
    "#### **Tiny Example: Requests + Beautiful Soup**\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://example.com\"\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "titles = [tag.text for tag in soup.find_all(\"h2\")]\n",
    "```\n",
    "\n",
    "> Pattern: download → parse → select → extract → clean\n",
    "\n",
    "Not every page has the same structure, so scraping often involves inspection and adaptation.\n",
    "\n",
    "**Bonus Trick:** Scraping Tables with pandas\n",
    "Pandas can parse HTML tables directly:\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df_list = pd.read_html(\"https://example.com/table\")\n",
    "df_list[0].head()\n",
    "```\n",
    "\n",
    "If the page contains \"< table > ... < /table >\" tags, this often works instantly; no tagging, no loops, no manual parsing.\n",
    "\n",
    "---\n",
    "### **BONUS Pandas Example: Scraping Population Data from an HTML Table**\n",
    "Suppose you want to collect the latest population estimates for U.S. states. The site has no public API, but shows an HTML table visible in the browser; a perfect case for scraping.\n",
    "\n",
    "The page might display data like this:\n",
    "\n",
    "| State        | 2024 Population | Growth Rate | Area (sq mi) | Density (per sq mi) |\n",
    "|-------------|-----------------|-------------|--------------|---------------------|\n",
    "| California  | 39,074,000      | -0.59%      | 163,695      | 240                 |\n",
    "| Texas       | 31,161,000      | +1.44%      | 268,597      | 116                 |\n",
    "| Florida     | 23,655,000      | +1.37%      | 65,758       | 345                 |\n",
    "| …           | …               | …           | …            | …                   |\n",
    "\n",
    "Because this data lives inside HTML `<table>` tags, we can use `requests` to download the page, `BeautifulSoup` to parse the HTML, and then convert the table into a Pandas `DataFrame`. Here is the conceptual Python snippet that accomplishes this:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Fetch the HTML\n",
    "url = \"https://worldpopulationreview.com/states\"\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# 2. Parse with BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 3. Find the main table on the page\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "# 4.1 Extract rows\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "# 4.2 Loop over the rows of the table\n",
    "data = []\n",
    "for row in rows:\n",
    "    # Get all header or data cells in this row\n",
    "    cells = row.find_all([\"td\", \"th\"])\n",
    "    # Extract the text and strip extra spaces\n",
    "    texts = [cell.get_text(strip=True) for cell in cells]\n",
    "    if texts:  # skip empty rows\n",
    "        data.append(texts)\n",
    "\n",
    "# 5. First row is the header, the rest is the body\n",
    "header = data[0]\n",
    "body = data[1:]\n",
    "\n",
    "# 6. Build a DataFrame from the scraped data\n",
    "df = pd.DataFrame(body, columns=header)\n",
    "\n",
    "# Take a quick look\n",
    "df.head()\n",
    "```\n",
    "\n",
    "Once in a DataFrame `df`, the data becomes programmable: you can clean, convert types, sort, filter, and analyze.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why Scraping Is Fragile?**\n",
    "Scrapers often break because HTML is designed for humans, not for machines.\n",
    "Small changes can cause failure:\n",
    "- renaming CSS classes\n",
    "- changing layout\n",
    "- adding ads or pop-ups\n",
    "- switching to JavaScript-rendered tables\n",
    "\n",
    "Contrast:\n",
    "- APIs are contracts\n",
    "- HTML is performance\n",
    "\n",
    "APIs are meant for machines; HTML is meant for browsers.\n",
    "\n",
    "### **Typical API Workflow**\n",
    "\n",
    "APIs remove the guesswork of scraping by returning structured data directly. The workflow is straightforward:\n",
    "\n",
    "1. Send a request (often a `GET`) to an API endpoint  \n",
    "2. Receive structured data (usually JSON)  \n",
    "3. Convert the JSON into a DataFrame for analysis\n",
    "\n",
    "**Example (conceptual):**\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://api.example.com/weather\")\n",
    "data = response.json()\n",
    "```\n",
    "This is the programmatic equivalent of asking a well-behaved machine for information.\n",
    "\n",
    "### **(b) RESTful APIs (Representational State Transfer)**\n",
    "RESTful APIs dominate modern data access: a client requests information, a server processes it, and structured data (often JSON) returns.\n",
    "\n",
    "> **RESTful API:** A style of API where data is organized as resources (e.g., /users, /repos) accessed using HTTP verbs such as GET, POST, PUT, and DELETE.\n",
    "\n",
    "<center> <img src=\"https://miro.medium.com/v2/1*f-4u01cDYiy6N5IRBktZnw.png\" width=\"400\"> <br> <em><strong>Figure 14.</strong> High-level overview of a RESTful API: structured request → structured response. Source: Medium</em> </center>\n",
    "\n",
    "\n",
    "#### **Tiny Example: Requests + RESTful APIs**\n",
    "\n",
    "The GitHub API is a public **RESTful API** that returns structured JSON data. Unlike scraping, no HTML parsing is required.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Send a GET request to a RESTful API endpoint\n",
    "url = \"https://api.github.com/repositories\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# 2. Parse the JSON response\n",
    "data = response.json()\n",
    "\n",
    "# 3. Convert JSON into a DataFrame\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "df.head()\n",
    "```\n",
    "\n",
    "For data science, the key takeaway is practical: RESTful APIs provide predictable, machine-readable access to fresh data. Modern dashboards, pipelines, and ML systems rely on them.\n",
    "\n",
    "\n",
    "> **A Note on Etiquette** Data availability is not the same as data permission. Good acquisition respects rate limits, terms of service, and the human effort behind the page or system being scraped.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNPYts+6JiGHcAfFnbpCyfm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
